# ğŸ“š AI Document Search Assistant

A powerful document search assistant built with Streamlit, LangChain, and HuggingFace LLM. Upload documents, ask questions, and get AI-powered answers based on your document content.

## âœ¨ Features

- **Multi-format Document Support**: Upload PDF, TXT, and DOCX files
- **Intelligent Text Processing**: Automatic text extraction and chunking
- **Semantic Search**: FAISS-powered vector similarity search
- **HuggingFace LLM Integration**: Powered by HuggingFace Inference API for intelligent responses
- **Chat Interface**: Conversational UI with chat history
- **Source Attribution**: View the document sources for each answer
- **Persistent Storage**: Vector store is saved locally for quick reloading

## ğŸ“ Project Structure

```
aidoc/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ document_loader.py     # Document loading (PDF, TXT, DOCX)
â”‚   â”œâ”€â”€ text_processor.py      # Text chunking and preprocessing
â”‚   â”œâ”€â”€ embeddings.py          # HuggingFace embedding generation
â”‚   â”œâ”€â”€ vector_store.py        # FAISS vector store operations
â”‚   â”œâ”€â”€ llm.py                 # HuggingFace LLM integration
â”‚   â””â”€â”€ qa_chain.py            # Question-answering chain
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vector_store/          # FAISS index storage
â”œâ”€â”€ uploads/                    # Uploaded documents (optional)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env.example               # Environment variables template
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- HuggingFace API key from [HuggingFace Settings](https://huggingface.co/settings/tokens)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/DeepakGuntupalli/AiDoc.git
   cd AiDoc
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**:
   ```bash
   # Copy the example file
   copy .env.example .env    # Windows
   cp .env.example .env      # macOS/Linux
   
   # Edit .env and add your HuggingFace API key
   ```

5. **Run the application**:
   ```bash
   streamlit run app.py
   ```

6. **Open your browser** and navigate to `http://localhost:8501`

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `HF_API_KEY` | Your HuggingFace API key (required) | - |
| `HF_MODEL` | HuggingFace model to use | `microsoft/Phi-3-mini-4k-instruct` |
| `EMBEDDING_MODEL` | HuggingFace embedding model | `all-MiniLM-L6-v2` |
| `VECTOR_STORE_PATH` | Path to store FAISS index | `./data/vector_store` |
| `CHUNK_SIZE` | Text chunk size | `1000` |
| `CHUNK_OVERLAP` | Overlap between chunks | `200` |

### Getting a HuggingFace API Key

1. Visit [HuggingFace](https://huggingface.co/)
2. Sign up or log in
3. Navigate to [Settings > Access Tokens](https://huggingface.co/settings/tokens)
4. Create a new access token
5. Copy the key to your `.env` file

## ğŸ“– Usage Guide

### Uploading Documents

1. Click on the file uploader in the sidebar
2. Select one or more documents (PDF, TXT, or DOCX)
3. Click "Process Documents" to extract and index the content
4. Wait for processing to complete

### Asking Questions

1. Type your question in the chat input at the bottom
2. Press Enter or click Send
3. The AI will search your documents and generate an answer
4. Click "View Sources" to see which document sections were used

### Tips for Best Results

- **Be specific**: Ask focused questions for more accurate answers
- **Upload related documents**: Group similar documents together
- **Use natural language**: Ask questions as you would to a human
- **Check sources**: Always verify answers against the source documents

## ğŸ”§ Technical Details

### Tech Stack

- **Frontend**: Streamlit
- **Backend**: Python
- **AI Framework**: LangChain
- **LLM**: HuggingFace Inference API
- **Vector Database**: FAISS
- **Embeddings**: HuggingFace Sentence Transformers

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚â”€â”€â”€â”€â–¶â”‚  Document       â”‚
â”‚   Frontend      â”‚     â”‚  Loader         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   QA Chain      â”‚â—€â”€â”€â”€â”€â”‚  Text           â”‚
â”‚   (LangChain)   â”‚     â”‚  Processor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HuggingFace   â”‚     â”‚  Embedding      â”‚
â”‚   LLM API       â”‚     â”‚  Manager        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  FAISS Vector   â”‚
                        â”‚  Store          â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Document Loading**: Documents are parsed and text is extracted
2. **Text Chunking**: Text is split into overlapping chunks for better context
3. **Embedding Generation**: Each chunk is converted to a vector embedding
4. **Vector Storage**: Embeddings are stored in FAISS for fast retrieval
5. **Query Processing**: User questions are embedded and matched against stored vectors
6. **Response Generation**: Relevant chunks are sent to HuggingFace LLM for answer generation

## â“ Troubleshooting

### Common Issues

**"HuggingFace API key is required"**
- Ensure your `.env` file exists and contains a valid `HF_API_KEY`
- Check that the API key is correct (no extra spaces)

**"Model not supported" error**
- Some models require a Pro subscription on HuggingFace
- Try using a different model like `microsoft/Phi-3-mini-4k-instruct`

**"Error loading embedding model"**
- First run may take time to download the model
- Ensure you have internet connection
- Try: `pip install --upgrade sentence-transformers`

**"Vector store not found"**
- Upload and process documents first
- Check that `./data/vector_store` directory exists

**PDF extraction issues**
- Ensure the PDF is not password protected
- Try a different PDF if text extraction fails

### Performance Tips

- Use smaller chunk sizes for more precise answers
- Increase chunk overlap for better context
- Limit the number of context chunks for faster responses

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## ğŸ‘¤ Author

**Deepak Guntupalli**

